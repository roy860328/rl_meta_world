{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: openai-whisper in /home/roytsai/.local/lib/python3.8/site-packages (20230314)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper) (4.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tiktoken==0.3.1 in /home/roytsai/.local/lib/python3.8/site-packages (from openai-whisper) (0.3.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/roytsai/.local/lib/python3.8/site-packages (from openai-whisper) (4.65.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/roytsai/.local/lib/python3.8/site-packages (from openai-whisper) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: triton==2.0.0 in /home/roytsai/.local/lib/python3.8/site-packages (from openai-whisper) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: torch in /home/roytsai/.local/lib/python3.8/site-packages (from openai-whisper) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numba in /home/roytsai/.local/lib/python3.8/site-packages (from openai-whisper) (0.56.4)\n",
      "Requirement already satisfied, skipping upgrade: ffmpeg-python==0.2.0 in /home/roytsai/.local/lib/python3.8/site-packages (from openai-whisper) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: regex>=2022.1.18 in /home/roytsai/.local/lib/python3.8/site-packages (from tiktoken==0.3.1->openai-whisper) (2023.3.23)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.26.0 in /home/roytsai/.local/lib/python3.8/site-packages (from tiktoken==0.3.1->openai-whisper) (2.28.2)\n",
      "Requirement already satisfied, skipping upgrade: lit in /home/roytsai/.local/lib/python3.8/site-packages (from triton==2.0.0->openai-whisper) (16.0.0)\n",
      "Requirement already satisfied, skipping upgrade: cmake in /home/roytsai/.local/lib/python3.8/site-packages (from triton==2.0.0->openai-whisper) (3.26.1)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->openai-whisper) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (10.2.10.91)\n",
      "Requirement already satisfied, skipping upgrade: networkx in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (3.0)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /usr/lib/python3/dist-packages (from torch->openai-whisper) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (11.4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (11.7.4.91)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (2.14.3)\n",
      "Requirement already satisfied, skipping upgrade: sympy in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (1.11.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (11.7.101)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (11.7.91)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/roytsai/.local/lib/python3.8/site-packages (from torch->openai-whisper) (10.9.0.58)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.9\" in /usr/lib/python3/dist-packages (from numba->openai-whisper) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from numba->openai-whisper) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: llvmlite<0.40,>=0.39.0dev0 in /home/roytsai/.local/lib/python3.8/site-packages (from numba->openai-whisper) (0.39.1)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/lib/python3/dist-packages (from ffmpeg-python==0.2.0->openai-whisper) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /home/roytsai/.local/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /usr/lib/python3/dist-packages (from nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->openai-whisper) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /home/roytsai/.local/lib/python3.8/site-packages (from sympy->torch->openai-whisper) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] roytsai 的密碼： \n"
     ]
    }
   ],
   "source": [
    "!sudo apt update && sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pytube\n",
    "\n",
    "from pytube import YouTube\n",
    "import os\n",
    "# 定義要建立的資料夾名稱\n",
    "folder_name = \"/home/roytsai/rl_meta_world\"\n",
    "\n",
    "# 定義要下載的影片 URL\n",
    "url = 'https://www.youtube.com/watch?v=mc9DuhK6D54'\n",
    "filename = 'PaLM-E.mp4'\n",
    "isVideo = True\n",
    "\n",
    "\n",
    "# 創建 YouTube 物件\n",
    "yt = YouTube(url)\n",
    "\n",
    "# 選擇要下載的影片格式\n",
    "if isVideo:\n",
    "    stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "else:\n",
    "    stream = yt.streams.filter(only_audio=True).first()\n",
    "\n",
    "# 開始下載影片，並指定檔名\n",
    "stream.download(output_path=f'{folder_name}/')\n",
    "\n",
    "# 重命名下載的檔案\n",
    "os.rename(f'{folder_name}/{stream.default_filename}', f'{folder_name}/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roytsai/virtual_env/pytorch-ml/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is Ritesh Riyan and welcome to my channel. In this video, let's have a first look at palm E, which is an embodied multi-model language model from Google Research, robotics at Google and TU Berlin. So what is this particular palm E? It is a multi-model language model. Previously I have covered Microsoft Cosmos. Now palm E is from Google. The idea is you have a large language model to which you can give different inputs like images. It could be embedding from other modalities like robot sensors. Interleaved with text, it can actually give you a text output which can be fed to a robot. The idea is given some embedding over here and an image. The question is how to grasp a blue block. And the answer is actually first grasp a low block. So given embedding of the sensors, which basically state over here. And this image, it has how to grasp blue block and the model actually gives the output in language, which can be then converted to control signals. This model, this large language model can also be used for visual question answering, captioning. It can be also used for language only tasks. Describe the following image basically. Given this comes under captioning. But under language only tasks, it is like, here is a high-core about embodied language models. And then you can have the model complete the text. Or it could be question answering. This model can also do mobile manipulation. So the humans is bring me rice chips from the drawer. The robot one, go to the drawer, open top drawer. I see. And then the image, basically this image. This is the output from the model which says, pick the green rice chip back from the drawer and place it on the counter. Okay. This is a task and motion planning, where in this table, it is about how to grasp the blue block. So for that, you need to first take the yellow block. So first grasp yellow block and place it on the table. Then grasp the blue block. And there is table top manipulation, where on a table top there are some objects. And you need to say, here, sort given this image, sort colors into corners. Then the model gives the output in steps, push the green star to the bottom left, push the green circle to the green star. Okay. That is how it does this table top manipulation. So various tasks are converted into a language only output, basically over here, by giving different kinds of inputs in the form of embeddings to this large language model. This concept is similar to what I have explained in Microsoft Cosmos one. There also we had a multi model, large language model, which was taking embeddings from, say, different things like images or audio and text. Here also it is the same thing. Okay. So the concept is similar. That's what I have realized over here. Okay. So what they say is that, Pami transfers knowledge from visual language domains into embodied reasoning from robot planning environments with complex dynamics and physical constraints, two answering questions about the observable world. Pami operates on multi model sentences, where you have a sequence of tokens, where inputs from, you know, arbitrary modalities, it could be images, it could be neural 3D representation or states. Okay. Inserted alongside text tokens. That is what they say over here. Basically these green and blue colors are multi model inputs. Orange ones are your text into a large language model, which has been trained in to end. That is a concept over here. They also have a project page where they have a lot of demos, basically on, you know, how this model performs a certain task. Okay. So for example, this is that example of bringing me the rice chips from the drawer. It has multiple planning steps and, you know, incorporating visual feedback from the robot's camera. Right. There is also another example for bringing me the green star. Okay. So these are example videos of how they have done it, basically. How the model gives certain outputs and based on that, it works out. Okay. Here how to sort the blocks by. Colors into the corners. That's the idea over here. So it first tries to push green to the one corner, then blue to the other corner. L.O. is a anyhow in that corner and red is in the corner. Right. So when again, if green is pushed over here, you can see that that it pushes back now red. This will kept over here. It tries to push red back. Right. So this is the two examples of generalization instruction is push red blocks to the coffee cup. This is another example. Where it has to push the red blocks to the coffee cup. Okay. Push green blocks to the turtle. So this is the red block and there is another example over here. Okay. So there are other examples of different use cases. For example, given this image, it is about can I go down this street on a bicycle? Yes or no. So here, if you look at this image, there is this accept. So it says that basically, bicycles cannot enter over here. Let's think step by step. So do not enter is there except bicycles. Do not enter except bicycle. So yes. So can I go down this street on a bicycle? Let's a question over here. So this image says do not enter except emergency and vehicles. So bicycles can enter this street. That is what is the inference over here. Okay. Let's look at some other example over here. Let's see what is this example? Given this image, who are the teams playing in this photo, which was the last to win a championship, which here they did. Okay. Team in white is New York. Green is Boston Celtic. Last win championship was Boston Celtic. Here was 2008. Okay. My star player was Paul Pierce. I'm not sure if this is correct. I'm not a basket ball fan. I just googled it. So this is the response from the model given this image. So basically, this is the input to the model. So many such examples are there over here. You can go and check out. Let's just quickly look into how this model was trained. Here are some more examples of visual condition jokes plus few short prompting zero short multi model chain of thought. This is the example which I talked about. Right. Physical prediction. Okay. Given this image, what will the robot do next? Whether it will fall or this thing. Okay. It says fall. Interesting. OCR free math reasoning. Okay. So today's special some pizza prices are there. Okay. I'm getting just two custom fees for me and my friend. How much should I pay in total? So it calculates that first step to custom pizzas. Then it goes over here. Create your own pizza price. It calculates and it gives out this thing. So it also has multi model chain of thought reasoning. Okay. This seems to be very similar to your Microsoft Sonos as well. That was a 1.6 billion parameter model. This is actually a 562 billion parameter model. Okay. So you have various palm E models also over here. Okay. I think I saw one example of the different sizes of models. Maybe let's quickly look at it. Where is that? I had seen somewhere about. Yeah. So there is palm E 12 billion. Palm E 84 billion. Okay. And yeah, the 564 billion model is also palm E 66 billion model also. Okay. Parameter models. Now let's quickly look at how they have you know what kind of features are extracted from various inputs. So the idea is from each input you basically have your what you call representations are converted into embeddings. Okay. For example, for robots, you have something called state vectors. So from a robot or state estimate for objects are simplest input to call me. So vector is mapped into a certain language embedding space. Okay. Um, so the state can contain post size color of those objects basically. For images vision transformer is converted into, you know, it maps an image into number of token embeddings. And that is therefore converted into some kind of using an affine transformation or something. It is converted into an input to the language model. Okay. I think that's what is explained over here. Yeah. For object centric representations, unlike language, visual input is not pre-structured into meaningful entity center relationships. VIT may capture semantics. The structure representation resembles a static grid rather than collection of object instances. We therefore also explore structured encoders that aim to separate visual inputs into distinct objects before giving them into LLM. So we given ground truth object instance mask. They can be decomposed into VIT representation. That is the idea over here. Okay. And then for object scenes, you have object scene representation transformer. Okay. So they make use of that because it doesn't rely on ground truth segmentations. Okay. And they convert that with an MLP. I think MLP stands for multiple. I'm sorry. Multi layer perceptron basically to convert. Yeah. The object representations into an embedding same with state estimation vectors over here. Okay. For entity reference for embodied planning tasks, Palmy must be able to reference objects in its generated plan. Okay. However, there are also existing settings where objects are not easily identical by a few words. For object centric representation, we label the multimodal tokens correspond to an object in the input prompt as. Okay. Now with special tokens, OBJJ. So that is how they mark over here. Object one is this. Okay. Right. So these are the different inputs which can be given into. Palmy and what is the main model behind palm? So they have used Palmy. So they have used Palm as their transformer model. Okay. Large language transfer model. As the base model, the difference is that the inputs can consist of text and multiple continuous observations. It could be multimodal sentences. Okay. A multimodal sentence example is this what happened between there is an image and image two where image i represents an embedding of the image. That is the input format. The output of Palmy is text which has been generated. So I find this very similar to the cosmos model because over there also you had some kind of image embeddings coming which has been converted into a language model kind of thing that they used clip or something. So here they have used a VIT or vision transformer. Okay. So that looks quite similar. So it is a decoder only. So the palm model is a decoder only large language model which tries to predict text given some input. So that is the these things. pre-trained model can be conditioned on a prefix. Okay. So you can basically use prompt to find in the models. What is token embedding space? I will discrete finite token embedding space. But here we need to actually map the multimodal things into this token embedding space. We train an encoder that maps a continuous observation space into a sequence of Q when you vector in X. These vectors are then interleaved with normal embedded text tokens basically. That is how to form the prefix for the large language model. Okay. Okay. So this is about how you are actually mapping this particular multiple multimodal vectors into this sentence. Okay. So how? Okay. Palm mean a robot control look. Okay. The palm is a generative model producing text based on this input. Okay. You connect to the thing we distinguish two cases. If the task can be accomplished by outputting text only, embodied question and answering on the then the output of the model is directory considered solution. Okay. It is used to solve a robot control task. It generates text that conditions low level commands. Okay. In particular, we assume to have access to policies that can perform low level skills from small vocabulary and a successful plan from Palm. We must consist a sequence of such skills. Palm. We must determine on its own which skills are available based on the training data and prompt and no other mechanism is used to constrain. So this is basically about how do you do the robot manipulation and other tasks. So what they say over here is that this model is capable of generating low level step for that during training. It needs to be aware of what is this low level steps and you know how it needs to perform that with certain prompts. That is what they've explained over here. Okay. So this was a high level about Palm E. I'm not really going into other things over here. So probably you can read through this paper to understand more about the other tasks and the details. Okay. So they have given some examples of Palm E guiding a real robot through one shot and zero shot table top manipulation tasks. This was the rice chip basically go to the drawers open the top drawer take the rice chips out of the drawer. Human knocks the rice chips back into the drawer. So this is some kind of a vegetable disturbance. But still it tries to take the rice chip out of the drawer bring it to the user put it down. This is the example of moving the green object. So some examples are present over here. Let's go to the appendix and see where they've actually explained about okay again the same thing wrong. So this talks about this table talks about the dataset which they have used. To train this particular large language model. So they have used also something about this robot manipulation some datasets from their datasets. They have used this. Okay. So these are the environment test samples. This is for objects basically for that robot manipulation. Okay. So where you give example prompts like this given an image. And a target basically. Okay. Okay. So task and motion planning that is tamp training scenes for tamp environment contain three to five cube shape objects of different size colors and sample initial poses. So that is this particular example. Okay. And then this is actually treated as a weak visual question answering tasks. Okay. Given this image is the red object left right or center of the table. Okay. The red object is center. This is this is how the tamp dataset looks like which stands for task and motion planning. These are your other question answering datasets visual question answering datasets. Okay. What else is there? These are actually basically success rate on this particular environments. Okay. For these particular tasks compare it to other models. Okay. I'm just trying to see success rates of what different input representations. Okay. Not other models basically over here. Then this is for your language generation and understanding tasks. The performance of various palm E models. Okay. And here is the comparison between palm 540 billion parameters and palm E 562 billion parameters. So if you look at that, yeah, for some tasks palm is better for some tasks palm E B is better. That's what I've shown over here. Okay. Okay. Okay. So I'll be putting a link to this paper as well as this site. You can go and look over here and try to understand more about this particular paper. It's quite interesting. But this is a huge model. 546 billion parameters. 546 or 564. Which is this actually 562 billion parameter model. Okay. Whereas Cosmos 1 from Microsoft was 1.6 billion parameter models. Yeah. So you can go and check out this particular paper. I hope this video is useful for you. If you like the video, please like share, subscribe to the channel. See you in another video.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(f'{folder_name}/{filename}')\n",
    "print(result[\"text\"])\n",
    "\n",
    "with open(f'{folder_name}/temp.txt', \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-ml",
   "language": "python",
   "name": "pytorch-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
